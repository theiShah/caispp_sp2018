{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop you will be building a Convolutional neural network to classify cats vs dogs. You will need to be familiar with the theory of CNNs. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson7) for more info. Only fill in the TODO sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, make sure you have cv2 installed!\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "# DO NOT CHANGE ANY OF THIS\n",
    "\n",
    "DATA_PATH = './data/train/'\n",
    "TEST_PERCENT = 0.1\n",
    "# This is just for sake of time. In real situations of course you would use the whole dataset.\n",
    "SELECT_SUBSET_PERCENT = 0.25\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "# DO NOT CHANGE\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "# We are setting this to be 5 epochs for fast training times. In practice we would have many more epochs. \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Load the train and test data sets. Do not modify this code at all. Make sure that your data for cats and dogs images is in ``./data``. You can find that data at https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 6250 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:23: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Train set has dimensionality (5625, 32, 32, 3)\n",
      "Test set has dimensionality (625, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets get started by loading the data.\n",
    "# Make sure you have the data downloaded to ./data\n",
    "# To download the data go to https://www.kaggle.com/c/dogs-vs-cats/data and download train.zip\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "While not necessary for this problem you can go ahead and try some preprocessing steps to try to get higher accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO: (Optional)\n",
    "# Perform any data preprocessing steps\n",
    "\n",
    "\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "Here are some useful resources to help with defining a powerful network.\n",
    "- Convolution layers (use the 2D convolution) https://keras.io/layers/convolutional/\n",
    "- Batch norm layer https://keras.io/layers/normalization/\n",
    "- Layer initializers https://keras.io/initializers/\n",
    "- Dense layer https://keras.io/layers/core/#dense\n",
    "- Activation functions https://keras.io/layers/core/#activation\n",
    "- Regulizers: \n",
    "    - https://keras.io/layers/core/#dropout\n",
    "    - https://keras.io/regularizers/\n",
    "    - https://keras.io/callbacks/#earlystopping\n",
    "    - https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "######################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define the network\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "######################################\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64)) # Densely connected neural network layer \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1)) # Final output layer - binary classification \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define your loss and your objective\n",
    "optimizer = 'rmsprop'\n",
    "loss = 'binary_crossentropy'\n",
    "######################################\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Time\n",
    "Train the network. Be on the lookout for the validation loss and accuracy. Don't change any of the parameters here except for the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 1125 samples\n",
      "Epoch 1/5\n",
      "4500/4500 [==============================] - 8s 2ms/step - loss: 0.6930 - acc: 0.5087 - val_loss: 0.6892 - val_acc: 0.5938\n",
      "Epoch 2/5\n",
      "4500/4500 [==============================] - 7s 2ms/step - loss: 0.6913 - acc: 0.5296 - val_loss: 0.6870 - val_acc: 0.5618\n",
      "Epoch 3/5\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 0.6890 - acc: 0.5342 - val_loss: 0.6966 - val_acc: 0.5040\n",
      "Epoch 4/5\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 0.7040 - acc: 0.4940 - val_loss: 0.7000 - val_acc: 0.4960\n",
      "Epoch 5/5\n",
      "4500/4500 [==============================] - 7s 1ms/step - loss: 0.6982 - acc: 0.5076 - val_loss: 0.6906 - val_acc: 0.5431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4354a6a20>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Define the batch size\n",
    "batch_size = select_count\n",
    "######################################\n",
    "\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time\n",
    "Now it's time to actually test the network. \n",
    "\n",
    "Get above **65%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 545us/step\n",
      "\n",
      "Got 55.20% accuracy\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
